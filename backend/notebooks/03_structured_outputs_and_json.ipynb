{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e18f643",
   "metadata": {},
   "source": [
    "# 03 - Structured Outputs and JSON\n",
    "\n",
    "## Objectives\n",
    "- JSON schema design for interview questions\n",
    "- Structured output formats (questions, answers, feedback)\n",
    "- Response parsing and validation\n",
    "- Error handling for malformed outputs\n",
    "\n",
    "## Expected Output\n",
    "JSON schemas and parsing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d39135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[JSON] Creating question schema\n",
      "Question Schema Structure:\n",
      "┌─ Required Fields:\n",
      "│  ✓ question (string)\n",
      "│  ✓ type (string)\n",
      "│  ✓ difficulty (string)\n",
      "│  ✓ duration_minutes (integer)\n",
      "├─ Optional Fields:\n",
      "│  • follow_ups (array)\n",
      "│  • evaluation_points (array)\n",
      "├─ Interview Types:\n",
      "│  1. technical\n",
      "│  2. behavioral\n",
      "│  3. system_design\n",
      "│  4. coding\n",
      "└─ Difficulty Levels:\n",
      "   1. junior\n",
      "   2. mid\n",
      "   3. senior\n",
      "   4. lead\n",
      "[SCHEMA] Question schema v1.0 created successfully\n",
      "[JSON] Creating response schema\n",
      "Response Schema Structure:\n",
      "┌─ Required Fields:\n",
      "│  ✓ response_text (string)\n",
      "│  ✓ overall_rating (string)\n",
      "│  ✓ feedback (object)\n",
      "├─ Feedback Structure:\n",
      "│  • summary (string)\n",
      "│  • strengths (array)\n",
      "│  • improvements (array)\n",
      "├─ Rating Scales:\n",
      "│  1. excellent\n",
      "│  2. good\n",
      "│  3. fair\n",
      "│  4. poor\n",
      "└─ Evaluation Categories:\n",
      "   1. technical_accuracy\n",
      "   2. communication\n",
      "   3. problem_solving\n",
      "   4. depth\n",
      "[SCHEMA] Response schema v1.0 created successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from core.llm_client import setup_openai_client\n",
    "from core.schemas import create_question_schema, create_response_schema\n",
    "from core.llm_integration import implement_structured_generation\n",
    "from utils.json_utils import implement_schema_validation, parse_llm_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb6bcdc",
   "metadata": {},
   "source": [
    "## Phase 1: Core Schema Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc3b5f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[JSON] Creating question schema\n",
      "Question Schema Structure:\n",
      "┌─ Required Fields:\n",
      "│  ✓ question (string)\n",
      "│  ✓ type (string)\n",
      "│  ✓ difficulty (string)\n",
      "│  ✓ duration_minutes (integer)\n",
      "├─ Optional Fields:\n",
      "│  • follow_ups (array)\n",
      "│  • evaluation_points (array)\n",
      "├─ Interview Types:\n",
      "│  1. technical\n",
      "│  2. behavioral\n",
      "│  3. system_design\n",
      "│  4. coding\n",
      "└─ Difficulty Levels:\n",
      "   1. junior\n",
      "   2. mid\n",
      "   3. senior\n",
      "   4. lead\n",
      "[SCHEMA] Question schema v1.0 created successfully\n"
     ]
    }
   ],
   "source": [
    "question_schema_config = create_question_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b73cc9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[JSON] Creating response schema\n",
      "Response Schema Structure:\n",
      "┌─ Required Fields:\n",
      "│  ✓ response_text (string)\n",
      "│  ✓ overall_rating (string)\n",
      "│  ✓ feedback (object)\n",
      "├─ Feedback Structure:\n",
      "│  • summary (string)\n",
      "│  • strengths (array)\n",
      "│  • improvements (array)\n",
      "├─ Rating Scales:\n",
      "│  1. excellent\n",
      "│  2. good\n",
      "│  3. fair\n",
      "│  4. poor\n",
      "└─ Evaluation Categories:\n",
      "   1. technical_accuracy\n",
      "   2. communication\n",
      "   3. problem_solving\n",
      "   4. depth\n",
      "[SCHEMA] Response schema v1.0 created successfully\n"
     ]
    }
   ],
   "source": [
    "response_schema_config = create_response_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7b9617",
   "metadata": {},
   "source": [
    "## Phase 2: Generation & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e49476f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SETUP] Initializing OpenAI client configuration\n",
      "[CONFIG] Loading environment variables from: ../.env\n",
      "[CONFIG] API key successfully retrieved from environment\n",
      "[SETUP] OpenAI client initialized successfully\n"
     ]
    }
   ],
   "source": [
    "client = setup_openai_client(\"../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "004f7abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[JSON] Starting structured generation with gpt-4o-mini\n",
      "[SUCCESS] Generated 6 fields, 293 tokens\n",
      "[TEST] Generated question: Explain the differences between lists, tuples, and sets in Python. In what scenarios would you choos...\n",
      "[COMPLETED] Phase 2.1 - Structured Generation\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"Generate a technical interview question about Python data structures for a mid-level developer.\"\n",
    "generation_result = implement_structured_generation(\n",
    "    client, \n",
    "    test_prompt, \n",
    "    question_schema_config[\"schema\"]\n",
    ")\n",
    "\n",
    "if generation_result[\"success\"]:\n",
    "    print(f\"[TEST] Generated question: {generation_result['data']['question'][:100]}...\")\n",
    "    \n",
    "print(\"[COMPLETED] Phase 2.1 - Structured Generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bac224c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[JSON] Creating schema validator\n",
      "[COMPLETED] Schema validator created for 4 required fields\n",
      "[TEST] Validation result: True\n",
      "[COMPLETED] Phase 2.2 - Schema Validation\n"
     ]
    }
   ],
   "source": [
    "question_validator = implement_schema_validation(question_schema_config)\n",
    "\n",
    "test_question = {\n",
    "    \"question\": \"What is the difference between == and === in JavaScript?\",\n",
    "    \"type\": \"technical\",\n",
    "    \"difficulty\": \"mid\",\n",
    "    \"duration_minutes\": 5,\n",
    "    \"follow_ups\": [\"Can you explain type coercion?\"],\n",
    "    \"evaluation_points\": [\"Technical accuracy\", \"Clear explanation\"]\n",
    "}\n",
    "\n",
    "validation_result = question_validator[\"validate_single\"](test_question)\n",
    "print(f\"[TEST] Validation result: {validation_result['is_valid']}\")\n",
    "\n",
    "print(\"[COMPLETED] Phase 2.2 - Schema Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8082a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PARSE] Processing LLM response of 77 characters\n",
      "[ERROR] JSON parsing failed: Expecting property name enclosed in double quotes: line 1 column 77 (char 76)\n",
      "[REPAIR] Attempting JSON repair\n",
      "[REPAIR] Attempting to fix malformed JSON\n",
      "[REPAIR] Applied fixes: trailing_commas, quote_keys\n",
      "[SUCCESS] JSON repaired successfully\n",
      "[WARNING] Schema validation failed: 'duration_minutes' is a required property\n",
      "\n",
      "Failed validating 'required' in schema:\n",
      "    {'type': 'obj...\n",
      "Parse result: SUCCESS\n",
      "[COMPLETED] Phase 3.1 - LLM Response Parsing\n"
     ]
    }
   ],
   "source": [
    "test_malformed = '{\"question\": \"What is Python?\", \"type\": \"technical\", \"difficulty\": \"junior\",}'  # trailing comma\n",
    "parse_result = parse_llm_responses(test_malformed, question_schema_config[\"schema\"])\n",
    "\n",
    "print(f\"Parse result: {'SUCCESS' if parse_result['success'] else 'FAILED'}\")\n",
    "print(\"[COMPLETED] Phase 3.1 - LLM Response Parsing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Interview Backend",
   "language": "python",
   "name": "interview-backend-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
