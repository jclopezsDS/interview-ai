{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ae2a94",
   "metadata": {},
   "source": [
    "# 05 - LangChain Agents and Chains\n",
    "\n",
    "## Objectives\n",
    "- Agent design for interview preparation\n",
    "- Chain composition for complex workflows\n",
    "- Memory management for conversation context\n",
    "- Tool integration and custom tools\n",
    "\n",
    "## Expected Output\n",
    "LangChain agent architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b67e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.langchain_setup import (\n",
    "    setup_langchain_environment,\n",
    "    create_interview_agent,\n",
    "    implement_state_management,\n",
    "    create_question_generation_chain,\n",
    "    implement_evaluation_chain,\n",
    "    create_follow_up_chain,\n",
    "    implement_custom_tools,\n",
    "    integrate_agent_toolkit,\n",
    "    implement_multi_agent_workflow,\n",
    "    create_conversation_workflow,\n",
    "    implement_persistent_memory,\n",
    "    optimize_agent_performance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "754e72c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LANGGRAPH] Initializing modern LangChain environment\n",
      "[CONFIG] API key loaded: sk-proj-...UY0A\n",
      "[SUCCESS] LangChain environment initialized\n",
      "[INFO] Model: gpt-4.1-nano | Temperature: 0.7 | Max tokens: 1000\n",
      "[TEST] Connection verified: 50 chars response\n"
     ]
    }
   ],
   "source": [
    "langchain_env = setup_langchain_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a49e6e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LANGGRAPH] Creating modern interview agent\n",
      "[SUCCESS] LangGraph agent created successfully\n",
      "[INFO] Tools count: 0 | Memory: MemorySaver | Model: gpt-4.1-nano\n"
     ]
    }
   ],
   "source": [
    "interview_agent = create_interview_agent(\n",
    "    llm=langchain_env[\"llm\"],\n",
    "    memory=langchain_env[\"memory\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36cf53dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LANGGRAPH] Initializing StateGraph with persistent context\n",
      "[STATE] Custom InterviewState schema configured\n",
      "[WORKFLOW] StateGraph nodes and edges configured\n",
      "[MEMORY] Checkpointer integrated with StateGraph\n",
      "[COMPLETED] StateGraph with MemorySaver configured successfully\n",
      "[INFO] Max messages: 50 | Persistent threads: Enabled\n",
      "[TEST] Thread state: {'thread_id': 'interview_session_001', 'messages_count': 0, 'question_count': 0, 'interview_stage': 'unknown', 'has_state': False}\n",
      "[TEST] Interview step result: True\n",
      "[TEST] Response length: 31 chars\n"
     ]
    }
   ],
   "source": [
    "state_system = implement_state_management(\n",
    "    llm=langchain_env[\"llm\"],\n",
    "    memory_saver=langchain_env[\"memory\"],\n",
    "    max_messages=50\n",
    ")\n",
    "\n",
    "test_thread = \"interview_session_001\"\n",
    "state_info = state_system[\"get_state_info\"](test_thread)\n",
    "print(f\"[TEST] Thread state: {state_info}\")\n",
    "\n",
    "test_result = state_system[\"run_step\"](\"Hello, I'm ready for the interview\", test_thread)\n",
    "print(f\"[TEST] Interview step result: {test_result['success']}\")\n",
    "print(f\"[TEST] Response length: {len(test_result['response'])} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e3afd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LANGGRAPH] Creating question generation chain with LCEL\n",
      "[CONFIG] Question types: 4 | Difficulty levels: 3\n",
      "[CHAIN] LCEL question generation chain created\n",
      "[COMPLETED] Question generation chain configured successfully\n",
      "[GENERATE] Technical question | Mid level | Python Developer\n",
      "[SUCCESS] Generated question: 213 chars\n",
      "[TEST] Generated question: 'How would you design a FastAPI endpoint that retrieves product information from a PostgreSQL database and caches the results in Redis to improve response times? Please outline your approach and key considerations.'\n",
      "[BATCH] Generating 2 questions\n",
      "[BATCH] Question 1/2 generated\n",
      "[BATCH] Question 2/2 generated\n",
      "[COMPLETED] Batch generation: 2 questions created\n",
      "[TEST] Batch results: 2 questions generated\n"
     ]
    }
   ],
   "source": [
    "question_gen_chain = create_question_generation_chain(\n",
    "    llm=langchain_env[\"llm\"],\n",
    "    question_types=[\"technical\", \"behavioral\", \"case_study\", \"problem_solving\"],\n",
    "    difficulty_levels=[\"junior\", \"mid\", \"senior\"]\n",
    ")\n",
    "\n",
    "test_question = question_gen_chain[\"generate_single\"](\n",
    "    question_type=\"technical\",\n",
    "    role=\"Python Developer\",\n",
    "    difficulty=\"mid\",\n",
    "    company_context=\"E-commerce startup\",\n",
    "    role_focus=\"API development\",\n",
    "    required_skills=\"FastAPI, PostgreSQL, Redis\"\n",
    ")\n",
    "\n",
    "print(f\"[TEST] Generated question: '{test_question}'\")\n",
    "\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"question_type\": \"behavioral\",\n",
    "        \"role\": \"Data Scientist\", \n",
    "        \"difficulty\": \"senior\",\n",
    "        \"company_context\": \"Financial services\",\n",
    "        \"role_focus\": \"ML model deployment\",\n",
    "        \"required_skills\": \"Python, MLOps, cloud platforms\"\n",
    "    },\n",
    "    {\n",
    "        \"question_type\": \"case_study\",\n",
    "        \"role\": \"Software Engineer\",\n",
    "        \"difficulty\": \"junior\", \n",
    "        \"company_context\": \"Healthcare tech\",\n",
    "        \"role_focus\": \"Web development\",\n",
    "        \"required_skills\": \"React, Node.js, databases\"\n",
    "    }\n",
    "]\n",
    "\n",
    "batch_questions = question_gen_chain[\"generate_batch\"](test_scenarios, max_questions=2)\n",
    "print(f\"[TEST] Batch results: {len(batch_questions)} questions generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "148717ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LANGGRAPH] Creating evaluation chain with structured outputs\n",
      "[CONFIG] Evaluation aspects: 4 | Max score: 10\n",
      "[CHAIN] LCEL evaluation chain with structured output created\n",
      "[COMPLETED] Evaluation chain configured successfully\n",
      "[EVALUATE] Processing response for Python Developer | Senior\n",
      "[SUCCESS] Evaluation completed | Overall score: 7/10\n",
      "[TEST] Evaluation result: Score 7/10\n",
      "[TEST] Strengths: 3 identified\n",
      "[TEST] Follow-up: Can you explain how the event loop works in Python's asyncio library and how it ...\n",
      "[BATCH] Evaluating 2 responses\n",
      "[BATCH] Processing evaluation 1/2\n",
      "[EVALUATE] Processing response for Frontend Developer | Mid-level\n",
      "[SUCCESS] Evaluation completed | Overall score: 8/10\n",
      "[BATCH] Processing evaluation 2/2\n",
      "[EVALUATE] Processing response for Backend Developer | Junior\n",
      "[SUCCESS] Evaluation completed | Overall score: 5/10\n",
      "[COMPLETED] Batch evaluation: Average score 6.5/10\n",
      "[SUMMARY] 2 evaluations | Avg: 6.5\n",
      "[TEST] Batch evaluation completed: 2 responses\n",
      "[TEST] Pass rate: 50.0%\n"
     ]
    }
   ],
   "source": [
    "evaluation_system = implement_evaluation_chain(\n",
    "    llm=langchain_env[\"llm\"],\n",
    "    max_score=10,\n",
    "    evaluation_aspects=[\"technical_accuracy\", \"communication_clarity\", \"problem_solving\", \"code_quality\"]\n",
    ")\n",
    "\n",
    "test_question = \"Explain the difference between synchronous and asynchronous programming in Python.\"\n",
    "test_answer = \"Synchronous programming executes code sequentially, blocking until each operation completes. Asynchronous programming uses async/await to handle multiple operations concurrently without blocking, improving performance for I/O operations.\"\n",
    "\n",
    "evaluation_result = evaluation_system[\"evaluate_single\"](\n",
    "    question=test_question,\n",
    "    answer=test_answer,\n",
    "    job_role=\"Python Developer\",\n",
    "    experience_level=\"Senior\"\n",
    ")\n",
    "\n",
    "print(f\"[TEST] Evaluation result: Score {evaluation_result['overall_score']}/10\")\n",
    "print(f\"[TEST] Strengths: {len(evaluation_result['strengths'])} identified\")\n",
    "print(f\"[TEST] Follow-up: {evaluation_result['follow_up_question'][:80]}...\")\n",
    "\n",
    "\n",
    "batch_data = [\n",
    "    {\n",
    "        \"question\": \"What is a closure in JavaScript?\",\n",
    "        \"answer\": \"A closure is a function that has access to variables in its outer scope even after the outer function returns.\",\n",
    "        \"job_role\": \"Frontend Developer\",\n",
    "        \"experience_level\": \"Mid-level\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Explain REST API principles.\",\n",
    "        \"answer\": \"REST uses HTTP methods like GET, POST, PUT, DELETE to perform operations on resources identified by URLs.\",\n",
    "        \"job_role\": \"Backend Developer\", \n",
    "        \"experience_level\": \"Junior\"\n",
    "    }\n",
    "]\n",
    "\n",
    "batch_results = evaluation_system[\"evaluate_batch\"](batch_data)\n",
    "summary = evaluation_system[\"generate_summary\"](batch_results)\n",
    "\n",
    "print(f\"[TEST] Batch evaluation completed: {summary['total_evaluations']} responses\")\n",
    "print(f\"[TEST] Pass rate: {summary['pass_rate']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb3d15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LANGGRAPH] Creating dynamic follow-up generation chain\n",
      "[CONFIG] Context window: 3 exchanges\n",
      "[CONFIG] Difficulty adaptation: True\n",
      "[COMPLETED] Dynamic follow-up chain configured successfully\n",
      "[FOLLOWUP] Processing response of 159 characters\n",
      "[SUCCESS] Generated 2 follow-up questions\n",
      "[QUALITY] Response assessed as: weak (score: 2)\n",
      "[TEST] Generated follow-ups: ['Can you tell me more about the specific features you used for the logistic regression model and how you selected them?', \"What were some of the challenges you faced during this project, and how did you address them to improve your model's performance?\"]\n",
      "[TEST] Quality score: 2\n",
      "[TEST] Difficulty level: easy\n"
     ]
    }
   ],
   "source": [
    "follow_up_system = create_follow_up_chain(\n",
    "    llm=langchain_env[\"llm\"],\n",
    "    context_window=3,\n",
    "    difficulty_adaptation=True\n",
    ")\n",
    "\n",
    "test_result = follow_up_system[\"generate_follow_up\"](\n",
    "    original_question=\"Tell me about a challenging project you worked on recently.\",\n",
    "    candidate_response=\"I worked on a machine learning project to predict customer churn. We used Python and scikit-learn to build a logistic regression model with about 85% accuracy.\",\n",
    "    conversation_history=[]\n",
    ")\n",
    "\n",
    "print(f\"[TEST] Generated follow-ups: {test_result['follow_up_questions']}\")\n",
    "print(f\"[TEST] Quality score: {test_result['quality_assessment']['quality_score']}\")\n",
    "print(f\"[TEST] Difficulty level: {test_result['difficulty_level']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37ca43ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LANGGRAPH] Creating custom interview tools\n",
      "[SUCCESS] Created 4 custom interview tools\n",
      "[INFO] Tools: difficulty_adjuster, topic_validator, timing_tracker, follow_up_generator\n",
      "[TEST] Validating tool functionality\n",
      "[TEST] adjust_difficulty: Difficulty: medium | Reason: Weak response - decre...\n",
      "[TEST] validate_topic: Relevance: low | Score: 35.0% | Topics: 1/2 | Role...\n",
      "[TEST] track_timing: Status: on_track | Elapsed: 5.0min | Avg/Q: 5.0min...\n",
      "[TEST] follow_up: Follow-up: Can you walk me through a specific exam...\n"
     ]
    }
   ],
   "source": [
    "custom_tools_config = implement_custom_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71876ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LANGGRAPH] Integrating agent toolkit with error handling\n",
      "[TOOLKIT] Combining 4 custom tools\n",
      "[INFO] Available tools: adjust_difficulty, validate_topic_relevance, track_interview_timing, generate_follow_up_suggestion\n",
      "[SUCCESS] LangGraph ReAct agent created successfully\n",
      "[TEST] Testing integrated agent toolkit\n",
      "[EXEC] Attempt 1: Processing user input\n",
      "[SUCCESS] Agent executed successfully on attempt 1\n",
      "[TEST] Integration test: SUCCESS\n",
      "[HEALTH] Validating toolkit health\n",
      "[HEALTH] Toolkit health: healthy (100.0% tools operational)\n",
      "[EXEC] Attempt 1: Processing user input\n",
      "[SUCCESS] Agent executed successfully on attempt 1\n",
      "Agent Response: Yes, I can help you practice Python coding questions. Let's start with a basic one:\n",
      "\n",
      "Write a Python function that takes a list of integers and returns the sum of all even numbers in the list.\n",
      "\n",
      "Would you like to try solving this, or should I provide a solution and explanation?\n",
      "Execution Success: True\n"
     ]
    }
   ],
   "source": [
    "integrated_toolkit = integrate_agent_toolkit(\n",
    "    llm=langchain_env[\"llm\"],\n",
    "    custom_tools=custom_tools_config[\"tools\"],\n",
    "    memory=langchain_env[\"memory\"]\n",
    ")\n",
    "\n",
    "agent_executor = integrated_toolkit[\"agent_executor\"]\n",
    "execute_agent = integrated_toolkit[\"execute_agent\"]\n",
    "capabilities = integrated_toolkit[\"capabilities\"]\n",
    "health_status = integrated_toolkit[\"health_status\"]\n",
    "\n",
    "test_result = execute_agent(\n",
    "    user_input=\"I'm preparing for a Python Developer interview. Can you help me practice?\",\n",
    "    session_id=\"interview_session_001\"\n",
    ")\n",
    "\n",
    "print(f\"Agent Response: {test_result['response']}\")\n",
    "print(f\"Execution Success: {test_result['success']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a921920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LANGGRAPH] Creating multi-agent workflow with StateGraph\n",
      "[SUCCESS] Multi-agent workflow created successfully\n",
      "[AGENT] Coordinator node managing interview flow\n",
      "[AGENT] Questioner node generating technical question\n",
      "[AGENT] Coordinator node managing interview flow\n",
      "[AGENT] Questioner node generating technical question\n",
      "[AGENT] Coordinator node managing interview flow\n",
      "[AGENT] Questioner node generating technical question\n",
      "[AGENT] Coordinator node managing interview flow\n",
      "[AGENT] Evaluator node assessing candidate response\n",
      "[AGENT] Coordinator node managing interview flow\n",
      "[AGENT] Questioner node generating technical question\n",
      "[AGENT] Coordinator node managing interview flow\n",
      "Multi-agent test: SUCCESS\n",
      "Current question: How would you implement a Python function to remove duplicate entries from a list while preserving the original order of the elements?\n",
      "Feedback count: 1\n"
     ]
    }
   ],
   "source": [
    "multi_agent_workflow = implement_multi_agent_workflow(\n",
    "    llm=langchain_env[\"llm\"],\n",
    "    all_tools=integrated_toolkit[\"all_tools\"],\n",
    "    memory=langchain_env[\"memory\"]\n",
    ")\n",
    "\n",
    "multi_agent_app = multi_agent_workflow[\"multi_agent_app\"]\n",
    "execute_interview = multi_agent_workflow[\"execute_interview\"]\n",
    "\n",
    "\n",
    "test_result = execute_interview(\n",
    "    job_role=\"Python Developer\",\n",
    "    candidate_answer=\"I have 3 years of experience with Python and Django\"\n",
    ")\n",
    "\n",
    "print(f\"Multi-agent test: {'SUCCESS' if test_result['success'] else 'FAILED'}\")\n",
    "print(f\"Current question: {test_result['current_question']}\")\n",
    "print(f\"Feedback count: {len(test_result['feedback'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebf1f9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LANGGRAPH] Building complete interview conversation workflow\n",
      "[STATE] Defined comprehensive ConversationState with 12 fields\n",
      "[SUCCESS] Complete conversation workflow compiled\n",
      "[TEST] Testing conversation workflow\n",
      "[SETUP] Initializing interview for Python Developer\n",
      "[INTRO] Conducting interview introduction\n",
      "[TEST] Conversation test: SUCCESS\n",
      "[TEST] First question generated: 443 characters\n"
     ]
    }
   ],
   "source": [
    "conversation_workflow = create_conversation_workflow(\n",
    "    multi_agent_app=multi_agent_workflow[\"multi_agent_app\"],\n",
    "    all_tools=integrated_toolkit[\"all_tools\"],\n",
    "    llm=langchain_env[\"llm\"],\n",
    "    memory=langchain_env[\"memory\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b4fe373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LANGGRAPH] Implementing advanced persistent memory system\n",
      "[MEMORY] Defined persistent memory schemas\n",
      "[TEST] Testing persistent memory system\n",
      "[MEMORY] Saving session state: test_persistent_1760470071 (test_checkpoint)\n",
      "[SUCCESS] Session test_persistent_1760470071 checkpoint saved\n",
      "[MEMORY] Loading session state: test_persistent_1760470071\n",
      "[SUCCESS] Session test_persistent_1760470071 loaded successfully\n",
      "[ANALYTICS] Generating session analytics\n",
      "[SUCCESS] Analytics generated\n",
      "[TEST] Persistent memory test: SUCCESS\n"
     ]
    }
   ],
   "source": [
    "persistent_memory = implement_persistent_memory(\n",
    "    memory=langchain_env[\"memory\"],\n",
    "    llm=langchain_env[\"llm\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f9c75df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OPTIMIZE] Fine-tuning LangGraph execution for production\n",
      "[CONFIG] Production optimization parameters configured\n",
      "[MEMORY] Optimizing checkpointing strategy\n",
      "[SUCCESS] Memory checkpointing optimized\n",
      "[LLM] Optimizing model performance and efficiency\n",
      "[SUCCESS] LLM performance optimized\n",
      "[MONITOR] Setting up performance monitoring\n",
      "[SUCCESS] Performance monitoring implemented\n",
      "[WRAPPER] Creating production deployment wrapper\n",
      "[MEMORY] Optimizing checkpointing strategy\n",
      "[SUCCESS] Memory checkpointing optimized\n",
      "[LLM] Optimizing model performance and efficiency\n",
      "[SUCCESS] LLM performance optimized\n",
      "[MONITOR] Setting up performance monitoring\n",
      "[SUCCESS] Performance monitoring implemented\n",
      "[SUCCESS] Production wrapper created\n",
      "[TEST] Testing production optimizations\n",
      "[SETUP] Initializing interview for Senior Python Developer\n",
      "[INTRO] Conducting interview introduction\n",
      "[TEST] Production optimization test: SUCCESS\n",
      "[PERFORMANCE] Total optimization test time: 2.689s\n"
     ]
    }
   ],
   "source": [
    "production_optimization = optimize_agent_performance(\n",
    "    conversation_app=conversation_workflow[\"conversation_app\"],\n",
    "    multi_agent_app=multi_agent_workflow[\"multi_agent_app\"],\n",
    "    all_tools=integrated_toolkit[\"all_tools\"],\n",
    "    memory=langchain_env[\"memory\"],\n",
    "    llm=langchain_env[\"llm\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Interview Backend",
   "language": "python",
   "name": "interview-backend-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
