{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fef91b0",
   "metadata": {},
   "source": [
    "# 04 - Multi-LLM Integration\n",
    "\n",
    "## Objectives\n",
    "- Integration with Gemini, Claude, etc.\n",
    "- LLM-as-judge implementation (LLM2 validates LLM1)\n",
    "- Cross-model comparison and consensus\n",
    "- Fallback mechanisms\n",
    "\n",
    "## Expected Output\n",
    "Multi-LLM orchestration patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e073c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "from src.core.llm_client import (\n",
    "    get_gemini_api_key,\n",
    "    setup_multi_llm_clients,\n",
    "    implement_provider_fallback,\n",
    "    create_judge_prompts,\n",
    "    implement_cross_validation,\n",
    "    implement_dual_llm_consensus,\n",
    "    create_cost_optimizer,\n",
    "    export_multi_llm_pipeline,\n",
    "    run_system_validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34227e74",
   "metadata": {},
   "source": [
    "## Phase 1: Multi-Provider Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ec6750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] Gemini API key loaded and configured successfully\n",
      "[INFO] First 5 characters of API key: AIzaS*****\n",
      "[INIT] Loading environment variables from: ../.env\n",
      "[SUCCESS] OpenAI client initialized\n",
      "[SETUP] Initializing Google Gemini client configuration\n",
      "[SUCCESS] Gemini API key loaded and configured successfully\n",
      "[INFO] First 5 characters of API key: AIzaS*****\n",
      "[SETUP] Google Gemini client initialized successfully\n",
      "[SUCCESS] Google Gemini client initialized\n",
      "[COMPLETED] Multi-LLM setup finished with 2 providers\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    gemini_api_key = get_gemini_api_key(env_filename=\"../.env\")\n",
    "    genai.configure(api_key=gemini_api_key)\n",
    "    print(\"[SUCCESS] Gemini API key loaded and configured successfully\")\n",
    "    print(f\"[INFO] First 5 characters of API key: {gemini_api_key[:5]}*****\")\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Failed to load or configure API key: {e}\")\n",
    "    gemini_api_key = None\n",
    "\n",
    "clients = setup_multi_llm_clients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0339c003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FALLBACK] Initializing fallback system with primary: openai\n",
      "[CONFIG] Fallback order: ['openai', 'google']\n",
      "[REQUEST] Starting request with fallback for prompt length: 69\n",
      "[ATTEMPT] Provider openai (attempt 1)\n",
      "[SUCCESS] openai responded in 4.47s\n",
      "[TEST] Response from openai: **Question:**\n",
      "\n",
      "You are given a list of integers that may contain duplicates. Write a Python function...\n"
     ]
    }
   ],
   "source": [
    "fallback_system = implement_provider_fallback(clients)\n",
    "test_response = fallback_system[\"make_request\"](\"Generate a technical interview question about Python data structures.\")\n",
    "print(f\"[TEST] Response from {test_response.provider.value}: {test_response.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95e8d5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[JUDGE] Creating evaluation prompts for LLM-as-judge system\n",
      "[COMPLETED] Created 3 judge prompt templates\n",
      "[CROSS-VALIDATION] Starting question_quality evaluation\n",
      "[REQUEST] Starting request with fallback for prompt length: 733\n",
      "[ATTEMPT] Provider openai (attempt 1)\n",
      "[SUCCESS] openai responded in 4.90s\n",
      "[SUCCESS] Cross-validation completed by openai\n",
      "[SCORE] Overall score: 8\n",
      "[EVALUATION] question_quality: 8/10\n"
     ]
    }
   ],
   "source": [
    "judge_prompts = create_judge_prompts()\n",
    "\n",
    "test_question = \"Explain the difference between a list and a tuple in Python, and provide use cases for each.\"\n",
    "evaluation = implement_cross_validation(\n",
    "    fallback_system,\n",
    "    judge_prompts,\n",
    "    test_question,\n",
    "    \"question_quality\"\n",
    ")\n",
    "print(f\"[EVALUATION] {evaluation['evaluation_type']}: {evaluation.get('evaluation_result', {}).get('overall_score', 'Failed')}/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebc227b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OPTIMIZER] Cost thresholds configured: {'openai_per_token': 0.0001, 'google_per_token': 0.0, 'max_cost_per_request': 0.1}\n",
      "[CONSENSUS] Starting dual-LLM consensus generation\n",
      "[GENERATE] Getting response from openai\n",
      "[SUCCESS] openai generated response (518 tokens)\n",
      "[GENERATE] Getting response from google\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760459692.385456 8738903 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SUCCESS] google generated response (1190 tokens)\n",
      "[CONSENSUS] Evaluating responses for best selection\n",
      "[REQUEST] Starting request with fallback for prompt length: 584\n",
      "[ATTEMPT] Provider openai (attempt 1)\n",
      "[SUCCESS] openai responded in 0.65s\n",
      "[JUDGE] Parsed result: {'winner': 'A', 'confidence': 8}\n",
      "[CONSENSUS] Selected openai with confidence 8/10\n",
      "[RESULT] Winner: openai | Cost: $0.0518\n"
     ]
    }
   ],
   "source": [
    "cost_optimizer = create_cost_optimizer()\n",
    "\n",
    "test_prompt = \"Create a challenging system design interview question about distributed databases.\"\n",
    "consensus_result = implement_dual_llm_consensus(fallback_system, judge_prompts, test_prompt)\n",
    "print(f\"[RESULT] Winner: {consensus_result.get('selected_provider', 'N/A')} | Cost: ${consensus_result.get('total_cost', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a48707ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EXPORT] Packaging multi-LLM system for production deployment\n",
      "[SUCCESS] Pipeline exported with 6 components\n",
      "[READY] System ready for src/ integration\n",
      "[VALIDATION] Running final system validation\n",
      "[TEST 1] Generate a Python coding interview quest...\n",
      "[CONSENSUS] Starting dual-LLM consensus generation\n",
      "[GENERATE] Getting response from openai\n",
      "[SUCCESS] openai generated response (304 tokens)\n",
      "[GENERATE] Getting response from google\n",
      "[SUCCESS] google generated response (971 tokens)\n",
      "[CONSENSUS] Evaluating responses for best selection\n",
      "[REQUEST] Starting request with fallback for prompt length: 584\n",
      "[ATTEMPT] Provider openai (attempt 1)\n",
      "[SUCCESS] openai responded in 0.76s\n",
      "[JUDGE] Parsed result: {'winner': 'A', 'confidence': 8}\n",
      "[CONSENSUS] Selected openai with confidence 8/10\n",
      "[RESULT] openai selected (confidence: 8/10)\n",
      "[TEST 2] Create a system design question about da...\n",
      "[CONSENSUS] Starting dual-LLM consensus generation\n",
      "[GENERATE] Getting response from openai\n",
      "[SUCCESS] openai generated response (514 tokens)\n",
      "[GENERATE] Getting response from google\n",
      "[SUCCESS] google generated response (1030 tokens)\n",
      "[CONSENSUS] Evaluating responses for best selection\n",
      "[REQUEST] Starting request with fallback for prompt length: 584\n",
      "[ATTEMPT] Provider openai (attempt 1)\n",
      "[SUCCESS] openai responded in 0.78s\n",
      "[JUDGE] Parsed result: {'winner': 'A', 'confidence': 8}\n",
      "[CONSENSUS] Selected openai with confidence 8/10\n",
      "[RESULT] openai selected (confidence: 8/10)\n",
      "[TEST 3] Design a behavioral question about teamw...\n",
      "[CONSENSUS] Starting dual-LLM consensus generation\n",
      "[GENERATE] Getting response from openai\n",
      "[SUCCESS] openai generated response (99 tokens)\n",
      "[GENERATE] Getting response from google\n",
      "[SUCCESS] google generated response (869 tokens)\n",
      "[CONSENSUS] Evaluating responses for best selection\n",
      "[REQUEST] Starting request with fallback for prompt length: 584\n",
      "[ATTEMPT] Provider openai (attempt 1)\n",
      "[SUCCESS] openai responded in 0.98s\n",
      "[JUDGE] Parsed result: {'winner': 'A', 'confidence': 8}\n",
      "[CONSENSUS] Selected openai with confidence 8/10\n",
      "[RESULT] openai selected (confidence: 8/10)\n",
      "[COMPLETED] Validation finished\n",
      "[METRICS] Success rate: 100% | Total cost: $0.0917\n",
      "\n",
      "==================================================\n",
      "MULTI-LLM INTEGRATION COMPLETED\n",
      "==================================================\n",
      "OpenAI + Google Gemini 2.5 Flash integration\n",
      "LLM-as-judge validation system\n",
      "Automatic fallback and cost optimization\n",
      "Production-ready export package\n",
      "Ready for src/core/llm_client.py integration\n"
     ]
    }
   ],
   "source": [
    "pipeline_export = export_multi_llm_pipeline()\n",
    "validation_results = run_system_validation(fallback_system, judge_prompts)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MULTI-LLM INTEGRATION COMPLETED\")\n",
    "print(\"=\"*50)\n",
    "print(\"OpenAI + Google Gemini 2.5 Flash integration\")\n",
    "print(\"LLM-as-judge validation system\")\n",
    "print(\"Automatic fallback and cost optimization\")\n",
    "print(\"Production-ready export package\")\n",
    "print(\"Ready for src/core/llm_client.py integration\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Interview Backend",
   "language": "python",
   "name": "interview-backend-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
