{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab181b22",
   "metadata": {},
   "source": [
    "# 06 - Vector Database RAG\n",
    "\n",
    "## Objectives\n",
    "- Vector database setup (Chroma/FAISS)\n",
    "- Job description embedding and retrieval\n",
    "- RAG pipeline for context-aware questions\n",
    "- Similarity search and relevance scoring\n",
    "\n",
    "## Expected Output\n",
    "RAG system for job-specific preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3f13965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import nltk\n",
    "\n",
    "from src.core.rag_pipeline import (\n",
    "    VectorStoreConfig, setup_hybrid_vector_store,\n",
    "    implement_adaptive_embeddings,\n",
    "    create_semantic_chunking,\n",
    "    implement_coarse_retrieval,\n",
    "    create_fine_grained_filtering,\n",
    "    build_metadata_fusion,\n",
    "    setup_reranking_model,\n",
    "    implement_listwise_reranking,\n",
    "    optimize_reranking_batching,\n",
    "    create_context_compression,\n",
    "    implement_relevance_filtering,\n",
    "    build_context_fusion,\n",
    "    implement_aggressive_caching,\n",
    "    optimize_retrieval_latency,\n",
    "    create_fallback_strategies\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9da47e",
   "metadata": {},
   "source": [
    "## Phase 1: Advanced Vector Foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a19ccd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] Initializing hybrid vector store configuration\n",
      "[RAG] FAISS IVF index created: 100 clusters, 1536D\n",
      "[RAG] Chroma collection ready: 0 documents\n",
      "[SUCCESS] Hybrid vector store configured successfully\n"
     ]
    }
   ],
   "source": [
    "config = VectorStoreConfig(\n",
    "    faiss_index_path=\"./data/faiss_index.idx\",\n",
    "    chroma_persist_dir=\"./data/chroma_db\",\n",
    "    embedding_dim=1536,\n",
    "    n_clusters=100,\n",
    "    metadata_fields=[\"job_role\", \"seniority\", \"domain\", \"company_size\"]\n",
    ")\n",
    "\n",
    "faiss_index, chroma_collection = setup_hybrid_vector_store(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9448d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] Initializing adaptive embeddings system\n",
      "[RAG] Fallback model loaded: BAAI/bge-large-en-v1.5\n",
      "[RAG] Embedding 2 texts with adaptive strategy\n",
      "[RAG] Primary embeddings completed: 0.96s, (2, 3072)\n",
      "[RAG] Test embeddings shape: (2, 3072)\n",
      "[SUCCESS] Adaptive embeddings system ready\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "adaptive_embedder = implement_adaptive_embeddings(openai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4092d58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] Initializing semantic chunking system\n",
      "[RAG] Semantic chunker initialized with all-MiniLM-L6-v2\n",
      "[RAG] Test chunking: 5 chunks created\n",
      "[RAG] Chunk 1: 195 chars\n",
      "[RAG] Chunk 2: 256 chars\n",
      "[RAG] Chunk 3: 224 chars\n",
      "[RAG] Chunk 4: 193 chars\n",
      "[RAG] Chunk 5: 184 chars\n",
      "[SUCCESS] Semantic chunking system ready\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "semantic_chunker = create_semantic_chunking()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae45a371",
   "metadata": {},
   "source": [
    "## Phase 2: Hierarchical Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a259787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] Initializing coarse retrieval system\n",
      "[RAG] Embedding 1 texts with adaptive strategy\n",
      "[RAG] Primary embeddings completed: 0.35s, (1, 3072)\n",
      "[RAG] Detected embedding dimension: 3072\n",
      "[RAG] Index created: 3072D Flat index (will upgrade to IVF)\n",
      "[RAG] Coarse retriever initialized: 0 vectors, 3072D\n",
      "[RAG] Embedding 4 texts with adaptive strategy\n",
      "[RAG] Primary embeddings completed: 0.45s, (4, 3072)\n",
      "[RAG] Added 4 documents, total: 4\n",
      "[RAG] Embedding 1 texts with adaptive strategy\n",
      "[RAG] Primary embeddings completed: 0.33s, (1, 3072)\n",
      "[RAG] Coarse retrieval: 3 candidates in 0.019s\n",
      "[RAG] Test query results: 3 matches\n",
      "[RAG] Match 1: score=0.656, doc='Senior Python Developer with machine learning expe...'\n",
      "[RAG] Match 2: score=0.719, doc='Data Scientist position open for predictive analyt...'\n",
      "[RAG] Match 3: score=1.250, doc='DevOps Engineer with Kubernetes and AWS experience...'\n",
      "[SUCCESS] Coarse retrieval system ready\n"
     ]
    }
   ],
   "source": [
    "coarse_retriever = implement_coarse_retrieval(adaptive_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8422d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] Initializing fine-grained filtering system\n",
      "[RAG] Fine-grained filter initialized: cosine similarity\n",
      "[RAG] Fine-grained filtering: 3 candidates\n",
      "[RAG] Embedding 4 texts with adaptive strategy\n",
      "[RAG] Primary embeddings completed: 0.58s, (4, 3072)\n",
      "[RAG] Fine-grained filtering completed: 2 refined in 0.585s\n",
      "[RAG] Test refinement: 2 documents refined\n",
      "[RAG] Refined 1: idx=0, score=1.000, doc='Senior Python Developer with machine lea...'\n",
      "[RAG] Refined 2: idx=2, score=0.915, doc='Data Scientist position open for predict...'\n",
      "[SUCCESS] Fine-grained filtering system ready\n"
     ]
    }
   ],
   "source": [
    "fine_grained_filter = create_fine_grained_filtering(adaptive_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "540420e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] Initializing metadata fusion system\n",
      "[RAG] Metadata fusion initialized: sim=0.6, role=0.2\n",
      "[RAG] Metadata fusion: processing 3 candidates\n",
      "[RAG] Query metadata: {'role': 'senior', 'domain': 'ai', 'skills': ['python', 'ml']}\n",
      "[RAG] Rank 1: idx=0, fused=1.310 (sim=0.850, role=1.000, domain=1.000, skills=1.000)\n",
      "[RAG] Rank 2: idx=2, fused=0.900 (sim=0.680, role=1.000, domain=0.500, skills=0.333)\n",
      "[RAG] Rank 3: idx=1, fused=0.587 (sim=0.720, role=0.700, domain=0.100, skills=0.000)\n",
      "[RAG] Test fusion: 3 documents reranked\n",
      "[RAG] Original order: [0, 1, 2]\n",
      "[RAG] Fused order: [0, 2, 1]\n",
      "[RAG] Score improvement: 0.460\n",
      "[SUCCESS] Metadata fusion system ready\n"
     ]
    }
   ],
   "source": [
    "metadata_fusion = build_metadata_fusion(coarse_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6198eaee",
   "metadata": {},
   "source": [
    "## Phase 3: Cross-Encoder Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c07b4f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] Initializing cross-encoder reranking system\n",
      "[RAG] Model loaded successfully: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "[RAG] Cross-encoder reranker initialized: cross-encoder/ms-marco-MiniLM-L-6-v2 on mps\n",
      "[RAG] Cross-encoder reranking: 3 passages\n",
      "[RAG] Cross-encoder reranking completed: 1 results in 0.038s\n",
      "[RAG] Rerank 1: idx=0, score=5.595\n",
      "[RAG] Test reranking: 1 passages reranked\n",
      "[RAG] Original order: [0, 1, 2]\n",
      "[RAG] Reranked order: [0]\n",
      "[RAG] Direct scoring test: 2 pairs scored\n",
      "[SUCCESS] Cross-encoder reranking system ready\n"
     ]
    }
   ],
   "source": [
    "cross_encoder_reranker = setup_reranking_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62ebee92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] Initializing listwise reranking system\n",
      "[RAG] Listwise reranker initialized: max_candidates=20, final_k=5\n",
      "[RAG] Listwise reranking: 6 candidates -> top 5\n",
      "[RAG] Listwise reranking completed: 5 results in 0.093s\n",
      "[RAG] Listwise 1: idx=0, score=0.919\n",
      "[RAG] Listwise 2: idx=5, score=1.185\n",
      "[RAG] Listwise 3: idx=4, score=2.232\n",
      "[RAG] Listwise 4: idx=2, score=3.359\n",
      "[RAG] Listwise 5: idx=1, score=4.044\n",
      "[RAG] Test listwise reranking: 5 final selections\n",
      "[RAG] Original candidates: 6\n",
      "[RAG] Final selection: [0, 5, 4, 2, 1]\n",
      "[SUCCESS] Listwise reranking system ready\n"
     ]
    }
   ],
   "source": [
    "listwise_reranker = implement_listwise_reranking(cross_encoder_reranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14d4be44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] Initializing optimized batch processing system\n",
      "[RAG] MPS optimization: moderate batching with threading\n",
      "[RAG] Batch processor initialized: device=mps, batch_size=8\n",
      "[RAG] Sequential batch processing: 8 pairs\n",
      "[RAG] Batch 0: 8 pairs processed in 0.033s\n",
      "[RAG] Sequential processing completed: 8 scores in 0.033s\n",
      "[RAG] Test batch processing: 8 scores computed\n",
      "[RAG] Score range: -11.351 to 5.595\n",
      "[RAG] Final batch size: 8\n",
      "[SUCCESS] Optimized batch processing system ready\n"
     ]
    }
   ],
   "source": [
    "optimized_batch_processor = optimize_reranking_batching(cross_encoder_reranker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52601afa",
   "metadata": {},
   "source": [
    "## Phase 4: Context Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d00a310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] Initializing context compression system\n",
      "[RAG] Context compressor initialized: model=gpt-4.1-nano, ratio=0.4\n",
      "[RAG] Compressing 3 contexts for query relevance\n",
      "[RAG] Compression completed: 148 -> 362 tokens (2.45 ratio) in 4.378s\n",
      "[RAG] Test compression: 791 -> 2180 chars\n",
      "[RAG] Compressing 1 contexts for query relevance\n",
      "[RAG] Compression completed: 55 -> 16 tokens (0.29 ratio) in 0.883s\n",
      "[RAG] Compressing 1 contexts for query relevance\n",
      "[RAG] Compression completed: 51 -> 16 tokens (0.31 ratio) in 0.478s\n",
      "[RAG] Compressing 1 contexts for query relevance\n",
      "[RAG] Compression completed: 42 -> 16 tokens (0.38 ratio) in 0.522s\n",
      "[RAG] Overlap compression: ratio=0.324, groups=3\n",
      "[SUCCESS] Context compression system ready\n"
     ]
    }
   ],
   "source": [
    "context_compressor = create_context_compression(openai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aafbe8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] Initializing relevance filtering system\n",
      "[RAG] Relevance filter initialized: method=adaptive, min_results=1\n",
      "[RAG] Filtering 7 documents by relevance\n",
      "[RAG] Using threshold: 0.323 (method: adaptive)\n",
      "[RAG] Applied diversity bonus: avg_diversity=0.826\n",
      "[RAG] Filtering completed: 7 -> 4 documents\n",
      "[RAG] Score range: 0.736 to 1.031\n",
      "[RAG] Test filtering: 7 -> 4 documents\n",
      "[RAG] Filtered indices: [0, 3, 1, 6]\n",
      "[RAG] Filtering 5 documents by relevance\n",
      "[RAG] Using threshold: 0.275 (method: adaptive)\n",
      "[RAG] Filtering completed: 5 -> 1 documents\n",
      "[RAG] Score range: 0.300 to 0.300\n",
      "[RAG] Low score test: 5 -> 1 documents\n",
      "[RAG] Threshold stats: adaptive=0.275\n",
      "[SUCCESS] Relevance filtering system ready\n"
     ]
    }
   ],
   "source": [
    "relevance_filter = implement_relevance_filtering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0217b122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] Initializing context fusion system\n",
      "[RAG] Context fusion initialized: strategy=hierarchical, overlap_threshold=0.3\n",
      "[RAG] Fusing 4 contexts using hierarchical strategy\n",
      "[RAG] Context fusion completed: 4 contexts -> 807 chars (compression: 1.004)\n",
      "[RAG] Average overlap detected: 0.075\n",
      "[RAG] Test fusion results:\n",
      "[RAG] Original contexts: 4\n",
      "[RAG] Compression ratio: 1.004\n",
      "[RAG] Average overlap: 0.075\n",
      "[RAG] Final text length: 807 characters\n",
      "[RAG] Fusing 3 contexts using hierarchical strategy\n",
      "[RAG] Context fusion completed: 3 contexts -> 172 chars (compression: 1.012)\n",
      "[RAG] Average overlap detected: 0.023\n",
      "[RAG] Diverse contexts test: overlap=0.023, compression=1.012\n",
      "[SUCCESS] Context fusion system ready\n"
     ]
    }
   ],
   "source": [
    "context_fusion = build_context_fusion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffe596c",
   "metadata": {},
   "source": [
    "## Phase 5: Production Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46db2f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] Initializing semantic caching system\n",
      "[RAG] Semantic cache initialized: threshold=0.95, ttl=3600s\n",
      "[RAG] Result cached for query\n",
      "[RAG] Semantic cache test: no match found (as expected for dissimilar queries)\n",
      "[SUCCESS] Semantic caching system ready\n"
     ]
    }
   ],
   "source": [
    "semantic_cache = implement_aggressive_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ced5adcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] Initializing retrieval latency optimization\n",
      "[RAG] Latency optimizer initialized: target=100.0ms\n",
      "[RAG] Warming up cache with 4 queries\n",
      "[RAG] Embedding 1 texts with adaptive strategy\n",
      "[RAG] Primary embeddings completed: 0.35s, (1, 3072)\n",
      "[RAG] Embedding 1 texts with adaptive strategy\n",
      "[RAG] Primary embeddings completed: 0.76s, (1, 3072)\n",
      "[RAG] Embedding 1 texts with adaptive strategy\n",
      "[RAG] Primary embeddings completed: 0.53s, (1, 3072)\n",
      "[RAG] Embedding 1 texts with adaptive strategy\n",
      "[RAG] Primary embeddings completed: 0.34s, (1, 3072)\n",
      "[RAG] Cache warmup completed\n",
      "[RAG] Testing latency optimization...\n",
      "[RAG] Embedding 1 texts with adaptive strategy\n",
      "[RAG] Primary embeddings completed: 0.57s, (1, 3072)\n",
      "[RAG] Embedding 1 texts with adaptive strategy\n",
      "[RAG] Primary embeddings completed: 0.52s, (1, 3072)\n",
      "[RAG] Coarse retrieval: 4 candidates in 0.000s\n",
      "[RAG] Skipped fine-grained filtering: budget=-421.8ms\n",
      "[RAG] Pipeline latency: 1090.4ms (target: 100.0ms)\n",
      "[RAG] Query latency: 1090.4ms, results: 4\n",
      "[RAG] Embedding 1 texts with adaptive strategy\n",
      "[RAG] Primary embeddings completed: 1.14s, (1, 3072)\n",
      "[RAG] Embedding 1 texts with adaptive strategy\n",
      "[RAG] Primary embeddings completed: 1.64s, (1, 3072)\n",
      "[RAG] Coarse retrieval: 4 candidates in 0.001s\n",
      "[RAG] Skipped fine-grained filtering: budget=-1544.9ms\n",
      "[RAG] Pipeline latency: 2789.5ms (target: 100.0ms)\n",
      "[RAG] Query latency: 2789.5ms, results: 4\n",
      "[RAG] Performance summary:\n",
      "[RAG] Average latency: 1939.9ms\n",
      "[RAG] Cache hit rate: 0.0%\n",
      "[SUCCESS] Retrieval latency optimization ready\n"
     ]
    }
   ],
   "source": [
    "latency_optimizer = optimize_retrieval_latency(\n",
    "    adaptive_embedder, coarse_retriever, fine_grained_filter, semantic_cache\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "839e2b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RAG] Initializing fallback strategies system\n",
      "[RAG] Fallback strategies initialized: max_wait=500.0ms\n",
      "[RAG] Attempting primary retrieval\n",
      "[RAG] Primary retrieval successful: 1.3ms\n",
      "[RAG] Test 1 - Normal operation: 1 results via cached, 1.3ms\n",
      "[RAG] Attempting primary retrieval\n",
      "[RAG] Primary retrieval failed: Vector store unavailable\n",
      "[RAG] Using cached fallback results\n",
      "[RAG] Using cached fallback: 0.3ms\n",
      "[RAG] Test 2 - With failure: 1 results via cached, 0.3ms\n",
      "[RAG] Attempting primary retrieval\n",
      "[RAG] Primary retrieval failed: Vector store unavailable\n",
      "[RAG] Using cached fallback results\n",
      "[RAG] Using cached fallback: 0.7ms\n",
      "[RAG] Test 3 - Cached fallback: 1 results via cached, 0.7ms\n",
      "[RAG] Attempting primary retrieval\n",
      "[RAG] Primary retrieval failed: Vector store unavailable\n",
      "[RAG] Using emergency response fallback\n",
      "[RAG] Test 4 - Emergency response: 3 results via emergency, 0.2ms\n",
      "[RAG] System status: 1 cached entries, healthy: True\n",
      "[SUCCESS] Fallback strategies system ready\n"
     ]
    }
   ],
   "source": [
    "fallback_strategies = create_fallback_strategies()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Interview Backend",
   "language": "python",
   "name": "interview-backend-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
